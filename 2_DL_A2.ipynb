{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bMCbQWK1sg0",
        "outputId": "910b57b5-6787-4f37-b16d-832d34f93cad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:\n",
            " [[-0.00209206]\n",
            " [-0.01032812]\n",
            " [-0.12894254]\n",
            " [-0.0043917 ]\n",
            " [-0.03779074]\n",
            " [-0.01886833]\n",
            " [-0.07123298]\n",
            " [-0.00313492]\n",
            " [-0.00554591]\n",
            " [-0.00162661]]\n",
            "\n",
            "True Prices:\n",
            " [[798]\n",
            " [831]\n",
            " [517]\n",
            " [168]\n",
            " [877]\n",
            " [748]\n",
            " [351]\n",
            " [778]\n",
            " [672]\n",
            " [356]]\n",
            "\n",
            "Mean Squared Error Loss: 425862.9337429743\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "#Dense Layer class\n",
        "class Layer_Dense:\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        #Weights & biases\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        #Computing the output\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "#ReLU\n",
        "class Activation_ReLU:\n",
        "    def forward(self, inputs):\n",
        "        #Applying ReLU\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "#Linear activation function (for regression)\n",
        "class Activation_Linear:\n",
        "    def forward(self, inputs):\n",
        "        self.output = inputs\n",
        "\n",
        "#Loss function - MSE\n",
        "class Loss_MeanSquaredError:\n",
        "    def calculate(self, y_pred, y_true):\n",
        "        #Calculate the mean squared error\n",
        "        loss = np.mean((y_true - y_pred) ** 2)\n",
        "        return loss\n",
        "\n",
        "#Datapoints\n",
        "X = np.array([\n",
        "    [136, 1, 359, 9],\n",
        "    [492, 3, 367, 34],\n",
        "    [630, 4, 126, 28],\n",
        "    [293, 2, 388, 46],\n",
        "    [340, 2, 158, 30],\n",
        "    [254, 2, 147, 23],\n",
        "    [689, 3, 350, 23],\n",
        "    [176, 1, 370, 40],\n",
        "    [439, 2, 338, 29],\n",
        "    [110, 1, 256, 7]\n",
        "])\n",
        "\n",
        "y = np.array([798, 831, 517, 168, 877, 748, 351, 778, 672, 356]).reshape(-1, 1)\n",
        "\n",
        "#NN architecture\n",
        "dense1 = Layer_Dense(4, 2)\n",
        "activation1 = Activation_ReLU()\n",
        "dense2 = Layer_Dense(2, 1)\n",
        "activation2 = Activation_Linear()\n",
        "\n",
        "#Loss function\n",
        "loss_function = Loss_MeanSquaredError()\n",
        "\n",
        "#Forward pass\n",
        "dense1.forward(X)\n",
        "activation1.forward(dense1.output)\n",
        "dense2.forward(activation1.output)\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "#Calculating Loss\n",
        "predictions = activation2.output\n",
        "loss = loss_function.calculate(predictions, y)\n",
        "\n",
        "#Results\n",
        "print(\"Predictions:\\n\", predictions)\n",
        "print(\"\\nTrue Prices:\\n\", y)\n",
        "print(\"\\nMean Squared Error Loss:\", loss)"
      ]
    }
  ]
}