{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S02MAWTUSKwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "069070ee-2e4b-4d9b-fec0-f4831870117d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.7048 - loss: 0.5330 - val_accuracy: 0.8624 - val_loss: 0.3220\n",
            "Epoch 2/3\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - accuracy: 0.9038 - loss: 0.2498 - val_accuracy: 0.8628 - val_loss: 0.3245\n",
            "Epoch 3/3\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.9296 - loss: 0.1857 - val_accuracy: 0.8566 - val_loss: 0.3598\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8562 - loss: 0.3570\n",
            "\n",
            "Test Accuracy: 0.8566\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.93      0.87     12500\n",
            "           1       0.91      0.79      0.85     12500\n",
            "\n",
            "    accuracy                           0.86     25000\n",
            "   macro avg       0.86      0.86      0.86     25000\n",
            "weighted avg       0.86      0.86      0.86     25000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[11570   930]\n",
            " [ 2654  9846]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "Review: The movie was absolutely wonderful, touching, and well-acted\n",
            "Predicted Sentiment: Negative (Confidence: 0.67)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "Review: Terrible plot, horrible acting. I regret watching it.\n",
            "Predicted Sentiment: Negative (Confidence: 0.96)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "Review: It was okay, not great but not bad either.\n",
            "Predicted Sentiment: Negative (Confidence: 0.81)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "#load IMDb data\n",
        "vocab_size = 10000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "#parameters\n",
        "embedding_dim = 128\n",
        "max_len = 200\n",
        "\n",
        "#pad sequences\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "\n",
        "#build model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
        "    LSTM(units=64),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "#compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#train model\n",
        "model.fit(X_train, y_train, batch_size=128, epochs=3, validation_data=(X_test, y_test))\n",
        "\n",
        "#evaluate\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "#predict and evaluate\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "#Raw Text Sentiment Prediction\n",
        "\n",
        "\n",
        "#load word index mapping\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# Adjust word index for special tokens\n",
        "word_index = {k: (v + 3) for k, v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "#function to encode and pad raw text\n",
        "def encode_review(text, word_index, max_len):\n",
        "    tokens = text.lower().split()\n",
        "    encoded = [1]\n",
        "    for word in tokens:\n",
        "        idx = word_index.get(word, 2)\n",
        "        encoded.append(idx)\n",
        "    padded = pad_sequences([encoded], maxlen=max_len)\n",
        "    return padded\n",
        "\n",
        "#function to predict sentiment from raw text\n",
        "def predict_sentiment(text, model, word_index, max_len):\n",
        "    encoded_review = encode_review(text, word_index, max_len)\n",
        "    prediction = model.predict(encoded_review)[0][0]\n",
        "    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
        "    confidence = prediction if prediction > 0.5 else 1 - prediction\n",
        "    print(f\"Review: {text}\")\n",
        "    print(f\"Predicted Sentiment: {sentiment} (Confidence: {confidence:.2f})\")\n",
        "    return sentiment, confidence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sample_reviews = [\n",
        "    \"The movie was absolutely wonderful, touching, and well-acted\",\n",
        "    \"Terrible plot, horrible acting. I regret watching it.\",\n",
        "    \"It was okay, not great but not bad either.\",\n",
        "]\n",
        "\n",
        "for review in sample_reviews:\n",
        "    predict_sentiment(review, model, word_index, max_len)\n",
        "\n"
      ]
    }
  ]
}